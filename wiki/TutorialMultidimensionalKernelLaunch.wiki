#summary Launching kernels in higher dimensions.

= Multidimensional Kernel Launch =

The [TutorialGlobalFunctions last section] hinted that `__global__` functions might be launched using grid configurations that weren't strictly one-dimensional.  In fact, if it is convenient for describing our data parallel problem to solve, CUDA allows us to create thread blocks in 1-, 2-, or 3D.  Many problems, such as the previous section's array processing tasks, are most naturally described in a flat, linear style mimicking our mental model of C's memory layout. Other tasks, particularly those often encountered in the computational sciences, are naturally embedded in two or three dimensions.  For example, [http://www.youtube.com/results?search_query=image+processing+cuda&page=&utm_source=opensearch image processing] tasks typically impose a regular 2D raster (an image) over the problem domain.  [http://www.youtube.com/results?search_query=cuda+cfd&search_type=&aq=f Computational fluid dynamics] tasks might be most naturally expressed by partitioning a volume over a 3D grid.

The following code listing demonstrates an example of a 2D kernel launch, and shows how to map two dimensional thread and block indices to the physical one dimensional layout of device memory.

{{{
#include <stdlib.h>
#include <stdio.h>

__global__ void kernel(int *array)
{
  int index_x = blockIdx.x * blockDim.x + threadIdx.x;
  int index_y = blockIdx.y * blockDim.y + threadIdx.y;

  // map the two 2D indices to a single linear, 1D index
  int grid_width = gridDim.x * blockDim.x;
  int index = index_y * grid_width + index_x;

  // map the two 2D block indices to a single linear, 1D block index
  int result = blockIdx.y * gridDim.x + blockIdx.x;

  // write out the result
  array[index] = result;
}

int main(void)
{
  int num_elements_x = 16;
  int num_elements_y = 16;

  int num_bytes = num_elements_x * num_elements_y * sizeof(int);

  int *device_array = 0;
  int *host_array = 0;

  // allocate memory in either space
  host_array = (int*)malloc(num_bytes);
  cudaMalloc((void**)&device_array, num_bytes);

  // create two dimensional 4x4 thread blocks
  dim3 block_size;
  block_size.x = 4;
  block_size.y = 4;

  // configure a two dimensional grid as well
  dim3 grid_size;
  grid_size.x = num_elements_x / block_size.x;
  grid_size.y = num_elements_y / block_size.y;

  // grid_size & block_size are passed as arguments to the triple chevrons as usual
  kernel<<<grid_size,block_size>>>(device_array);

  // download and inspect the result on the host:
  cudaMemcpy(host_array, device_array, num_bytes, cudaMemcpyDeviceToHost);

  // print out the result element by element
  for(int row = 0; row < num_elements_y; ++row)
  {
    for(int col = 0; col < num_elements_x; ++col)
    {
      printf("%2d ", host_array[row * num_elements_x + col]);
    }
    printf("\n");
  }
  printf("\n");

  // deallocate memory
  free(host_array);
  cudaFree(device_array);
}
}}}

The host code of this example uses a type we haven't seen before: `dim3`.  In order to configure a two-dimensional grid, we pass values of this type to the triple chevrons instead of the plain integer arguments we've used before for one-dimensional kernels.  `dim3` is a simple vector type, and you can assume its definition looks something like:

{{{
  struct dim3
  {
    unsigned int x, y, z;
    ...
  };
}}}

You'll note that our example didn't set either of `block_size.z` or `grid_size.z`.  By default, they are initialized to `1`.  Though the dimensionality of our problem is 2D, notice we configured the launch using arithmetic analogous to  our previous 1D examples.  First, we described the total size of the data parallel task at hand:

{{{
  int num_elements_x = 16;
  int num_elements_y = 16;
}}}

Next, we decided how to partition the task into blocks of threads:

{{{
  dim3 block_size;
  block_size.x = 4;
  block_size.y = 4;
}}}

And finally, we decided how many blocks of threads were needed to cover the size of the problem:

{{{
  dim3 grid_size;
  grid_size.x = num_elements_x / block_size.x;
  grid_size.y = num_elements_y / block_size.y;
}}}